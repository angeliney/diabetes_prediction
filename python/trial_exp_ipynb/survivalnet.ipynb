{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data - run prep_exp.ipynb first\n",
    "import pickle\n",
    "\n",
    "pickle_in = open(\"temp.pkl\",\"rb\")\n",
    "data = pickle.load(pickle_in)\n",
    "\n",
    "# # If it's not there, do the following:\n",
    "# import sys\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from str2bool import str2bool\n",
    "# from rpy2.robjects.packages import STAP\n",
    "# import rpy2.robjects as robjects\n",
    "# from rpy2.robjects import pandas2ri, numpy2ri\n",
    "# from rpy2.robjects.lib.dplyr import DataFrame\n",
    "# from rpy2.robjects.packages import importr\n",
    "\n",
    "# pandas2ri.activate()\n",
    "# numpy2ri.activate()\n",
    "\n",
    "# # Some temporary arguments for testing\n",
    "# include_lab = str2bool(\"T\")  # Include lab features?\n",
    "# include_ethdon = str2bool(\"T\")  # Include ethnicity + donor details?\n",
    "# lag = int(\"1\")  # Number of lag variables\n",
    "# eq_train_ratio = str2bool(\"T\")  # Train on equal case:control ratio?\n",
    "# __file__ = '/h/angeliney/projects/SRTR/prep_exp.py'\n",
    "# visit_type = \"first\"\n",
    "# output = \"temp\"\n",
    "# post2000 = True\n",
    "\n",
    "# # Get features based on these inputs\n",
    "# with open(os.path.join(os.path.dirname(__file__), 'features.R'), 'r') as f:\n",
    "#     string = f.read()\n",
    "# features_file = STAP(string, \"features\")\n",
    "# features_to_use = features_file.features.rx2(\"clin\")\n",
    "# if include_lab:\n",
    "#     features_to_use = features_to_use + features_file.features.rx2(\"lab\")\n",
    "\n",
    "# if include_ethdon:\n",
    "#     features_to_use = features_to_use + features_file.features.rx2(\"eth\") + features_file.features.rx2(\"don\")\n",
    "\n",
    "# timedep_cols = np.intersect1d(features_to_use, features_file.timedep_features)\n",
    "# cov_cols = np.setdiff1d(features_to_use, timedep_cols)\n",
    "\n",
    "# if eq_train_ratio:\n",
    "#     eq_cases_train_cols = np.array([\"TRR_ID\", \"is_diab\"])\n",
    "# else:\n",
    "#     eq_cases_train_cols = np.array()\n",
    "    \n",
    "# # Read RDS files (load data table)\n",
    "# readRDS = robjects.r['readRDS']\n",
    "# tx_li_study = readRDS(os.path.join(os.path.dirname(__file__), 'tx_li_formatted.rds'))\n",
    "# txf_li_study = readRDS(os.path.join(os.path.dirname(__file__), 'txf_li_formatted.rds'))\n",
    "\n",
    "# # Merge them\n",
    "# with open(os.path.join(os.path.dirname(__file__), 'functions.R'), 'r') as f:\n",
    "#     string = f.read()\n",
    "# functions = STAP(string, \"functions\")\n",
    "\n",
    "# merged = functions.combine_tx_txf(tx_li_study, txf_li_study, np.setdiff1d(cov_cols, \"age\"), timedep_cols, lag)\n",
    "\n",
    "# df = pandas2ri.ri2py_dataframe(DataFrame(merged).filter('time_next_followup > time_since_transplant'))\n",
    "\n",
    "# #Prep data for model training\n",
    "# cols = np.concatenate((timedep_cols, cov_cols))\n",
    "# if lag > 0:\n",
    "#     for l in range(1,  lag + 1):\n",
    "#         cols = np.append(cols, list(map(lambda x: '{}_{}'.format(x, l), timedep_cols)))\n",
    "\n",
    "# subset_cols = np.concatenate((['transplant_year', 'TRR_ID', 'age'], cols, ['is_diab', 'time_since_transplant',\n",
    "#                                                                    'time_next_followup', 'time_to_diab',\n",
    "#                                                                    'diab_time_since_tx', 'diab_in_1_year',\n",
    "#                                                                    'diab_now']))\n",
    "# df = df.dropna(subset=subset_cols)\n",
    "# df_test = df[(df.transplant_year.astype(int) >= 2011) & (df.time_to_diab >= 0)]\n",
    "# df_nontest = df[(df.transplant_year.astype(int) < 2011) & (df.time_to_diab >= 0)]\n",
    "\n",
    "# num_folds = 5\n",
    "# nontest_y = df_nontest.drop_duplicates(subset=['TRR_ID', 'is_diab']).is_diab\n",
    "# caret = importr('caret')\n",
    "# folds = caret.createFolds(nontest_y, num_folds, False)\n",
    "\n",
    "# data = {'test': df_test, 'train': df_nontest, 'cols': cols, 'eq_cases_train_cols': eq_cases_train_cols,\n",
    "#             'folds': folds}\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# # Save data in case kernel got restarted\n",
    "# pickle.dump(data, open(\"temp.pkl\", \"wb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "#import Bayesian_Optimization as BayesOpt\n",
    "import os\n",
    "import scipy.io as sio\n",
    "from survivalnet.optimization import SurvivalAnalysis\n",
    "import numpy as np\n",
    "from survivalnet.train import train\n",
    "import theano\n",
    "import cPickle\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some settings\n",
    "visit_type=\"random\"\n",
    "if visit_type == \"last\":\n",
    "    time_col = \"time_since_transplant\"\n",
    "    cols = data['cols']\n",
    "else:\n",
    "    time_col = \"time_to_diab\"\n",
    "    cols = np.append(data['cols'], 'time_since_transplant')\n",
    "    \n",
    "eq_train_ratio = True\n",
    "\n",
    "# Subset train, val, test\n",
    "nontest_ids = data['train'].drop_duplicates(subset=['TRR_ID', 'is_diab']).TRR_ID\n",
    "i=1\n",
    "train_ids = nontest_ids[np.array(data['folds']) != i]\n",
    "val_ids = np.setdiff1d(nontest_ids, train_ids)\n",
    "\n",
    "train = data['train'][data['train'].TRR_ID.isin(train_ids)]\n",
    "train = train[train.is_diab == 1]  # Only include censored objects in training. Otherwise, comment out\n",
    "val = data['train'][data['train'].TRR_ID.isin(val_ids)] \n",
    "val = val[val.is_diab == 1]  # Only include censored objects in training. Otherwise, comment out\n",
    "test = data['test']\n",
    "\n",
    "from functions import equalize_num_case_control, filter_train_by_visit\n",
    "train = filter_train_by_visit(visit_type, data['train'])\n",
    "train = equalize_num_case_control(train, data['eq_cases_train_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_bayes_opt = False\n",
    "epochs = 10\n",
    "output_path = \"results\"\n",
    "opt = 'GDLS'\n",
    "pretrain_config = None\n",
    "\n",
    "# The results in the paper are averaged over 20 random assignment of samples\n",
    "# to training/validation/testing sets.\n",
    "cindex_results_train =[]\n",
    "cindex_results =[]\n",
    "\n",
    "# Caclulates the risk group for every patient i: patients whose time of\n",
    "# death is greater than that of patient i.\n",
    "sa = SurvivalAnalysis()\n",
    "train_set = {}\n",
    "val_set = {}\n",
    "test_set = {}\n",
    "train_set['X'], train_set['T'], train_set['O'], train_set['A'] = sa.calc_at_risk(\n",
    "    train[cols].values.astype(\"float32\"),\n",
    "    train[time_col].values.astype(\"float32\"),\n",
    "    train.is_diab.values.astype(\"int32\"))\n",
    "\n",
    "val_set['X'], val_set['T'], val_set['O'], val_set['A'] = sa.calc_at_risk(\n",
    "    val[cols].values.astype(\"float32\"),\n",
    "    val[time_col].values.astype(\"float32\"),\n",
    "    val.is_diab.values.astype(\"int32\"))\n",
    "\n",
    "test_set['X'], test_set['T'], test_set['O'], test_set['A'] = sa.calc_at_risk(\n",
    "    test[cols].values.astype(\"float32\"),\n",
    "    test[time_col].values.astype(\"float32\"),\n",
    "    test.is_diab.values.astype(\"int32\"))\n",
    "\n",
    "pretrain_set = train_set['X']\n",
    "\n",
    "\n",
    "n_layers = 1\n",
    "n_hidden = 100\n",
    "do_rate = 0.5\n",
    "lambda1 = 0\n",
    "lambda2 = 0\n",
    "nonlin = np.tanh # or nonlin = theano.tensor.nnet.relu\n",
    "\n",
    "# Prints experiment identifier.\n",
    "expID = 'nl{}-hs{}-dor{}_nonlin{}_id{}'.format(str(n_layers), str(n_hidden), str(do_rate), str(nonlin), str(i))\n",
    "\n",
    "finetune_config = {'ft_lr': 0.0001, 'ft_epochs': epochs}\n",
    "\n",
    "#_, train_cindices, _, test_cindices, _, _, model, _ = train(pretrain_set, train_set, test_set, pretrain_config, \n",
    "#                                                            finetune_config, n_layers, n_hidden, \n",
    "#                                                            dropout_rate=do_rate, lambda1=lambda1, lambda2=lambda2, \n",
    "#                                                            non_lin=nonlin, optim=opt, verbose=True, earlystp=False)\n",
    "\n",
    "#print(train_cindices[np.argmax(test_cindices)])\n",
    "#print(max(test_cindices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "earlystp = False\n",
    "dropout_rate = do_rate\n",
    "non_lin = nonlin\n",
    "optim = opt\n",
    "\n",
    "import numpy\n",
    "import os\n",
    "import sys\n",
    "import theano\n",
    "import timeit\n",
    "\n",
    "from survivalnet.model import Model\n",
    "from survivalnet.optimization import BFGS, GDLS, SurvivalAnalysis, isOverfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE_DECAY = 1 \n",
    "finetune_lr = theano.shared(numpy.asarray(finetune_config['ft_lr'], dtype=theano.config.floatX))\n",
    "\n",
    "numpy_rng = numpy.random.RandomState(1111)\n",
    "\n",
    "# Construct the stacked denoising autoencoder and the corresponding\n",
    "# supervised survival network.\n",
    "model = Model(\n",
    "        numpy_rng = numpy_rng,\n",
    "        n_ins = train_set['X'].shape[1],\n",
    "        hidden_layers_sizes = [n_hidden] * n_layers,\n",
    "        n_outs = 1,\n",
    "        dropout_rate=dropout_rate,\n",
    "        lambda1 = lambda1,\n",
    "        lambda2 = lambda2,\n",
    "        non_lin=non_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-0b9d83df4c65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Calculates testing cost, risk and cindex using th eupdated model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtest_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_risk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtest_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtest_ci\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurvivalAnalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_risk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'T'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mtrain_cindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ci\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/h/angeliney/projects/SRTR/survivalnet/optimization/SurvivalAnalysis.py\u001b[0m in \u001b[0;36mc_index\u001b[0;34m(self, risk, T, C)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0mn_orderable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_orderable\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "########################\n",
    "# FINETUNING THE MODEL #\n",
    "########################\n",
    "test, train = model.build_finetune_functions(learning_rate=finetune_lr)\n",
    "\n",
    "train_cindices = []\n",
    "test_cindices = []\n",
    "train_costs = []\n",
    "test_costs = []\n",
    "\n",
    "if optim == 'BFGS':        \n",
    "    bfgs = BFGS(model, train_set['X'], train_set['O'], train_set['A'])\n",
    "elif optim == 'GDLS':\n",
    "    gdls = GDLS(model, train_set['X'], train_set['O'], train_set['A'])\n",
    "survivalAnalysis = SurvivalAnalysis()    \n",
    "\n",
    "# Starts the training routine.\n",
    "for epoch in range(finetune_config['ft_epochs']):\n",
    "\n",
    "    # Creates masks for dropout during training.\n",
    "    train_masks = [\n",
    "        numpy_rng.binomial(n=1, p=1-dropout_rate, \n",
    "                           size=(train_set['X'].shape[0], n_hidden))\n",
    "        for i in range(n_layers)]\n",
    "\n",
    "    # Creates dummy masks for testing.\n",
    "    test_masks = [\n",
    "        numpy.ones((test_set['X'].shape[0], n_hidden), dtype='int64')\n",
    "        for i in range(n_layers)]\n",
    "\n",
    "    # BFGS() and GDLS() update the gradients, so we only serve (test) the\n",
    "    # model to calculate cost, risk, and cindex on training set.\n",
    "    if optim == 'BFGS':        \n",
    "        bfgs.BFGS(train_masks)\n",
    "        train_cost, train_risk, train_features = test(\n",
    "            train_set['X'], train_set['O'], train_set['A'], 1, *train_masks)\n",
    "    elif optim == 'GDLS':        \n",
    "        gdls.GDLS(train_masks)\n",
    "        train_cost, train_risk, train_features = test(\n",
    "            train_set['X'], train_set['O'], train_set['A'], 1, *train_masks)\n",
    "    # In case of GD, uses the train function to update the gradients and get\n",
    "    # training cost, risk, and cindex at the same time.\n",
    "    elif optim == 'GD':\n",
    "        train_cost, train_risk, train_features = train(\n",
    "            train_set['X'], train_set['O'], train_set['A'], 1, *train_masks)\n",
    "    train_ci = survivalAnalysis.c_index(train_risk, train_set['T'], 1 - train_set['O'])\n",
    "\n",
    "    # Calculates testing cost, risk and cindex using th eupdated model.\n",
    "    test_cost, test_risk, _ = test(test_set['X'], test_set['O'], test_set['A'], 0, *test_masks)\n",
    "    test_ci = survivalAnalysis.c_index(test_risk, test_set['T'], 1 - test_set['O'])\n",
    "\n",
    "    train_cindices.append(train_ci)\n",
    "    test_cindices.append(test_ci)\n",
    "\n",
    "    train_costs.append(train_cost)\n",
    "    test_costs.append(test_cost)\n",
    "    if verbose: \n",
    "        print (('epoch = {}, trn_cost = {}, trn_ci = {}, tst_cost = {},'\n",
    "                ' tst_ci = {}').format(epoch, train_cost, train_ci,\n",
    "                                       test_cost, test_ci))\n",
    "    if earlystp and epoch >= 15 and (epoch % 5 == 0):\n",
    "        if verbose:\n",
    "            print 'Checking overfitting!'\n",
    "        check, max_iter = isOverfitting(numpy.asarray(test_cindices))\n",
    "        if check:                \n",
    "            print(('Training Stopped Due to Overfitting! cindex = {},'\n",
    "                   ' MaxIter = {}').format(test_cindices[max_iter], max_iter))\n",
    "            break\n",
    "    else: max_iter = epoch\n",
    "    sys.stdout.flush()\n",
    "    decay_learning_rate = theano.function(\n",
    "            inputs=[], outputs=finetune_lr,\n",
    "            updates={finetune_lr: finetune_lr * LEARNING_RATE_DECAY})    \n",
    "    decay_learning_rate()\n",
    "    epoch += 1\n",
    "    if numpy.isnan(test_cost): break \n",
    "if verbose: \n",
    "    print 'C-index score after {} epochs is: {}'.format(max_iter, max(test_cindices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, trn_cost = -54089.9651783, trn_ci = 0.497985618024, tst_cost = -68140.4049368, tst_ci = 0.471169146678\n"
     ]
    }
   ],
   "source": [
    "test_ci = survivalAnalysis.c_index(test_risk, test_set['T'], 1 - test_set['O'])\n",
    "\n",
    "train_cindices.append(train_ci)\n",
    "test_cindices.append(test_ci)\n",
    "\n",
    "train_costs.append(train_cost)\n",
    "test_costs.append(test_cost)\n",
    "if verbose: \n",
    "    print (('epoch = {}, trn_cost = {}, trn_ci = {}, tst_cost = {},'\n",
    "            ' tst_ci = {}').format(epoch, train_cost, train_ci,\n",
    "                                   test_cost, test_ci))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with file(os.path.join(output_path, 'final_model'), 'wb') as f:\n",
    "    cPickle.dump(model, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "outputFileName = os.path.join(output_path, 'c_index_list.csv')\n",
    "df = pandas.DataFrame(test_cindices)\n",
    "df.to_csv(outputFileName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
