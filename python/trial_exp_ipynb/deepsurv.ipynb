{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data - run prep_exp.ipynb first\n",
    "import pickle\n",
    "\n",
    "pickle_in = open(\"temp.pkl\", \"rb\")\n",
    "data = pickle.load(pickle_in)\n",
    "\n",
    "# # If it's not there, do the following:\n",
    "# import sys\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from str2bool import str2bool\n",
    "# from rpy2.robjects.packages import STAP\n",
    "# import rpy2.robjects as robjects\n",
    "# from rpy2.robjects import pandas2ri, numpy2ri\n",
    "# from rpy2.robjects.lib.dplyr import DataFrame\n",
    "# from rpy2.robjects.packages import importr\n",
    "\n",
    "# pandas2ri.activate()\n",
    "# numpy2ri.activate()\n",
    "\n",
    "# # Some temporary arguments for testing\n",
    "# include_lab = str2bool(\"T\")  # Include lab features?\n",
    "# include_ethdon = str2bool(\"T\")  # Include ethnicity + donor details?\n",
    "# lag = int(\"1\")  # Number of lag variables\n",
    "# eq_train_ratio = str2bool(\"T\")  # Train on equal case:control ratio?\n",
    "# __file__ = '/h/angeliney/projects/SRTR/prep_exp.py'\n",
    "# visit_type = \"first\"\n",
    "# output = \"temp\"\n",
    "# post2000 = True\n",
    "\n",
    "# # Get features based on these inputs\n",
    "# with open(os.path.join(os.path.dirname(__file__), 'features.R'), 'r') as f:\n",
    "#     string = f.read()\n",
    "# features_file = STAP(string, \"features\")\n",
    "# features_to_use = features_file.features.rx2(\"clin\")\n",
    "# if include_lab:\n",
    "#     features_to_use = features_to_use + features_file.features.rx2(\"lab\")\n",
    "\n",
    "# if include_ethdon:\n",
    "#     features_to_use = features_to_use + features_file.features.rx2(\"eth\") + features_file.features.rx2(\"don\")\n",
    "\n",
    "# timedep_cols = np.intersect1d(features_to_use, features_file.timedep_features)\n",
    "# cov_cols = np.setdiff1d(features_to_use, timedep_cols)\n",
    "\n",
    "# if eq_train_ratio:\n",
    "#     eq_cases_train_cols = np.array([\"TRR_ID\", \"is_diab\"])\n",
    "# else:\n",
    "#     eq_cases_train_cols = np.array()\n",
    "    \n",
    "# # Read RDS files (load data table)\n",
    "# readRDS = robjects.r['readRDS']\n",
    "# tx_li_study = readRDS(os.path.join(os.path.dirname(__file__), 'tx_li_formatted.rds'))\n",
    "# txf_li_study = readRDS(os.path.join(os.path.dirname(__file__), 'txf_li_formatted.rds'))\n",
    "\n",
    "# # Merge them\n",
    "# with open(os.path.join(os.path.dirname(__file__), 'functions.R'), 'r') as f:\n",
    "#     string = f.read()\n",
    "# functions = STAP(string, \"functions\")\n",
    "\n",
    "# merged = functions.combine_tx_txf(tx_li_study, txf_li_study, np.setdiff1d(cov_cols, \"age\"), timedep_cols, lag)\n",
    "\n",
    "# df = pandas2ri.ri2py_dataframe(DataFrame(merged).filter('time_next_followup > time_since_transplant'))\n",
    "\n",
    "# #Prep data for model training\n",
    "# cols = np.concatenate((timedep_cols, cov_cols))\n",
    "# if lag > 0:\n",
    "#     for l in range(1,  lag + 1):\n",
    "#         cols = np.append(cols, list(map(lambda x: '{}_{}'.format(x, l), timedep_cols)))\n",
    "\n",
    "# subset_cols = np.concatenate((['transplant_year', 'TRR_ID', 'age'], cols, ['is_diab', 'time_since_transplant',\n",
    "#                                                                    'time_next_followup', 'time_to_diab',\n",
    "#                                                                    'diab_time_since_tx', 'diab_in_1_year',\n",
    "#                                                                    'diab_now']))\n",
    "# df = df.dropna(subset=subset_cols)\n",
    "# df_test = df[(df.transplant_year.astype(int) >= 2011) & (df.time_to_diab >= 0)]\n",
    "# df_nontest = df[(df.transplant_year.astype(int) < 2011) & (df.time_to_diab >= 0)]\n",
    "\n",
    "# num_folds = 5\n",
    "# nontest_y = df_nontest.drop_duplicates(subset=['TRR_ID', 'is_diab']).is_diab\n",
    "# caret = importr('caret')\n",
    "# folds = caret.createFolds(nontest_y, num_folds, False)\n",
    "\n",
    "# data = {'test': df_test, 'train': df_nontest, 'cols': cols, 'eq_cases_train_cols': eq_cases_train_cols,\n",
    "#             'folds': folds}\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# # Save data in case kernel got restarted\n",
    "# pickle.dump(data, open(\"temp.pkl\", \"wb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try DeepSurv\n",
    "import numpy as np\n",
    "import pandas\n",
    "import sys\n",
    "import lasagne\n",
    "import deepsurv\n",
    "import optunity\n",
    "from str2bool import str2bool\n",
    "from functions import prep_exp, filter_train_by_visit\n",
    "\n",
    "\n",
    "nontest_ids = data['train'].drop_duplicates(subset=['TRR_ID', 'is_diab']).TRR_ID\n",
    "visit_type=\"random\"\n",
    "if visit_type == \"last\":\n",
    "    time_col = \"time_since_transplant\"\n",
    "    cols = data['cols']\n",
    "else:\n",
    "    time_col = \"time_to_diab\"\n",
    "    cols = np.append(data['cols'], 'time_since_transplant')\n",
    "\n",
    "    \n",
    "train = filter_train_by_visit(\"random\", data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best hyperparameter setting\n",
    "def get_objective_function(num_epochs, update_fn=lasagne.updates.adam):\n",
    "    \"\"\"\n",
    "    Returns the function for Optunity to optimize. The function returned by get_objective_function\n",
    "    takes the parameters: x_train, y_train, x_test, and y_test, and any additional kwargs to\n",
    "    use as hyper-parameters.\n",
    "    The objective function runs a DeepSurv model on the training data and evaluates it against the\n",
    "    test set for validation. The result of the function call is the validation concordance index\n",
    "    (which Optunity tries to optimize)\n",
    "    \"\"\"\n",
    "    def format_to_deepsurv(x, y):\n",
    "        return {\n",
    "            'x': x.astype(np.float32),\n",
    "            'e': y[:,0].astype(np.int32),\n",
    "            't': y[:,1].astype(np.float32)\n",
    "        }\n",
    "    \n",
    "    def get_hyperparams(params):\n",
    "        hyperparams = {\n",
    "            'batch_norm': True,\n",
    "            'standardize': True\n",
    "        }\n",
    "        if 'num_layers' in params and 'num_nodes' in params:\n",
    "            params['hidden_layers_sizes'] = [int(params['num_nodes'])] * int(params['num_layers'])\n",
    "            del params['num_layers']\n",
    "            del params['num_nodes']\n",
    "        if 'learning_rate' in params:\n",
    "            params['learning_rate'] = 10 ** params['learning_rate']\n",
    "        hyperparams.update(params)\n",
    "        return hyperparams\n",
    "    def train_deepsurv(x_train, y_train, x_test, y_test, **kwargs):\n",
    "        # Standardize the datasets\n",
    "        train_mean = x_train.mean(axis = 0)\n",
    "        train_std = x_train.std(axis = 0)\n",
    "        x_train = (x_train - train_mean) / train_std\n",
    "        x_test = (x_test - train_mean) / train_std\n",
    "\n",
    "        train_data = format_to_deepsurv(x_train, y_train)\n",
    "        valid_data = format_to_deepsurv(x_test, y_test)\n",
    "        \n",
    "        hyperparams = get_hyperparams(kwargs)\n",
    "\n",
    "        network = deepsurv.DeepSurv(n_in=train_data['x'].shape[1], **hyperparams)\n",
    "        metrics = network.train(train_data, n_epochs = num_epochs,\n",
    "            update_fn = update_fn, verbose = False)\n",
    "\n",
    "        result = network.get_concordance_index(**valid_data)\n",
    "        return result\n",
    "\n",
    "    return train_deepsurv\n",
    "\n",
    "update_fn = lasagne.updates.momentum\n",
    "opt_fxn = get_objective_function(100, update_fn=update_fn)\n",
    "\n",
    "\n",
    "\n",
    "box_constraints = {\n",
    "    \"learning_rate\": [-7, -3],\n",
    "    \"num_nodes\": [2, 20],\n",
    "    \"num_layers\": [1, 4],\n",
    "    \"lr_decay\": [0.0, 0.001],\n",
    "    \"momentum\": [0.8, 0.95],\n",
    "    \"L1_reg\": [0.05, 5.0],\n",
    "    \"L2_reg\": [0.05, 5.0],\n",
    "    \"dropout\": [0.0, 0.5]\n",
    "}\n",
    "\n",
    "opt_fxn = optunity.cross_validated(x=train[cols].values,\n",
    "                                   y=np.column_stack((train.is_diab.values, train[time_col].values)),\n",
    "                                   num_folds=5)(opt_fxn)\n",
    "opt_params, call_log, _ = optunity.maximize(opt_fxn, num_evals=100, solver_name='sobol', **box_constraints)\n",
    "hyperparams = opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pandas.DataFrame()\n",
    "i = 0\n",
    "if i > 0:\n",
    "    train_ids = nontest_ids[np.array(data['folds']) != i]\n",
    "    val_ids = np.setdiff1d(nontest_ids, train_ids)\n",
    "    test = data['train'][data['train']['TRR_ID'].isin(val_ids)]\n",
    "else:\n",
    "    train_ids = nontest_ids\n",
    "    test = data['test']\n",
    "\n",
    "train = filter_train_by_visit(visit_type, data['train'][data['train'].TRR_ID.isin(train_ids)])\n",
    "\n",
    "train_data = {\"x\": train[cols].values.astype(\"float32\"),\n",
    "              \"t\": train[time_col].values.astype(\"float32\"),\n",
    "              \"e\": train.is_diab.values.astype(\"int32\")}\n",
    "\n",
    "test_data = {\"x\": test[cols].values.astype(\"float32\"),\n",
    "             \"t\": test[time_col].values.astype(\"float32\"),\n",
    "             \"e\": test.is_diab.values.astype(\"int32\")}\n",
    "\n",
    "network = deepsurv.DeepSurv(**hyperparams)\n",
    "log = network.train(**train_data, n_epochs=100, update_fn=update_fn)\n",
    "train_cindex = network.get_concordance_index(**train_data)\n",
    "test_cindex = network.get_concordance_index(**test_data)\n",
    "\n",
    "# Get c-index case\n",
    "train_case_data = {\"x\": train.query('is_diab == 1')[cols].values.astype(\"float32\"),\n",
    "                   \"t\": train.query('is_diab == 1')[time_col].values.astype(\"float32\"),\n",
    "                   \"e\": train.query('is_diab == 1').is_diab.values.astype(\"int32\")}\n",
    "test_case_data = {\"x\": test.query('is_diab == 1')[cols].values.astype(\"float32\"),\n",
    "                  \"t\": test.query('is_diab == 1')[time_col].values.astype(\"float32\"),\n",
    "                  \"e\": test.query('is_diab == 1').is_diab.values.astype(\"int32\")}\n",
    "train_cindex_case = network.get_concordance_index(**train_case_data)\n",
    "test_cindex_case = network.get_concordance_index(**test_case_data)\n",
    "\n",
    "if i == 0:\n",
    "    perf = {'model': network, 'train_cindex': train_cindex, 'test_cindex': test_cindex,\n",
    "            'train_nrow': train.shape[0], 'test_nrow': test.shape[0],\n",
    "            'train_cindex_case': train_cindex_case, 'test_cindex_case': test_cindex_case}\n",
    "\n",
    "print('{},{},{},{},{},{},{}'.format(i, train_cindex, test_cindex, train.shape[0], test.shape[0], train_cindex_case,\n",
    "                                    test_cindex_case))\n",
    "result.append(pandas.DataFrame({'train_cindex': train_cindex, 'test_cindex': test_cindex,\n",
    "                                'train_nrow': train.shape[0], 'test_nrow': test.shape[0],\n",
    "                                'train_cindex_case': train_cindex_case, 'test_cindex_case': test_cindex_case},\n",
    "                               index=[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
